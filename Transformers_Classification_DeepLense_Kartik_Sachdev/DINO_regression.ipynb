{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip3 install --upgrade lightly einops vit-pytorch>=0.27.0 seaborn>=0.11.0 ipython thop gdown split-folders  protobuf==3.20.* e2cnn==0.1.9\n",
    "\n",
    "\n",
    "# scikit-learn \n",
    "# opencv-python>=4.1.1 Pillow>=8.2.0 PyYAML>=5.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray[tune] \\\n",
    "# ray[air] \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torchvision\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import *\n",
    "from config.data_config import DATASET\n",
    "from utils.dataset import DefaultDatasetSetupSSL\n",
    "from self_supervised.losses.contrastive_loss import (\n",
    "    ContrastiveLossEuclidean,\n",
    "    ContrastiveLossEmbedding,\n",
    "    SimCLR_Loss,\n",
    "    NegativeCosineSimilarity,\n",
    ")\n",
    "from self_supervised.losses.sym_neg_cos_sim_loss import SymNegCosineSimilarityLoss\n",
    "\n",
    "from models.modules.head import BYOLProjectionHead, BYOLPredictionHead\n",
    "from utils.scheduler import cosine_schedule\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from einops.layers.torch import Rearrange\n",
    "from config.cvt_config import CvT_CONFIG\n",
    "from models.transformer_zoo import TransformerModels\n",
    "import math\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from utils.inference import *\n",
    "from utils.trainer.finetune import finetune, finetune_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgParser(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.dataset_name = \"Model_II\"\n",
    "        self.save = \"data\"\n",
    "        self.batch_size = 64\n",
    "        self.epochs_pretrain = 15\n",
    "        self.epochs_finetune = 20 # TEMP\n",
    "        self.train_config = \"CvT\"\n",
    "        self.cuda = True\n",
    "        self.num_workers = 20\n",
    "        self.ci=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgParser()\n",
    "dataset_name = args.dataset_name\n",
    "dataset_dir = args.save\n",
    "batch_size = args.batch_size\n",
    "epochs_pretrain = args.epochs_pretrain\n",
    "epochs_finetune = args.epochs_finetune\n",
    "train_config_name = args.train_config\n",
    "use_cuda = args.cuda\n",
    "num_workers = args.num_workers\n",
    "ci = args.ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = CvT_CONFIG\n",
    "learning_rate = train_config[\"optimizer_config\"][\"lr\"]\n",
    "num_channels = train_config[\"channels\"]\n",
    "network_type = \"resnet50\" # train_config[\"network_type\"]\n",
    "image_size = train_config[\"image_size\"]\n",
    "optimizer_config = train_config[\"optimizer_config\"]\n",
    "log_dir_base = \"logger\"\n",
    "classes = [\"axion\"]\n",
    "num_classes = len(classes) # can resue as network o/p, since single class with regression\n",
    "\n",
    "make_directories([dataset_dir])\n",
    "seed_everything(seed=42)\n",
    "device = get_device(use_cuda=use_cuda, cuda_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "log_dir_name = f\"{current_time}_Regression_DINO_{network_type}_{dataset_name}\"\n",
    "\n",
    "\n",
    "log_dir = f\"{log_dir_base}/{log_dir_name}\"\n",
    "init_logging_handler(log_dir_base, log_dir_name)\n",
    "\n",
    "# paths\n",
    "model_path_pretrained = os.path.join(\n",
    "    f\"{log_dir}/checkpoint\",\n",
    "    f\"{network_type}_pretrained_{dataset_name}_{current_time}.pt\",\n",
    ")\n",
    "\n",
    "finetuned_model_path = os.path.join(\n",
    "    f\"{log_dir}/checkpoint\",\n",
    "    f\"{network_type}_finetune_{dataset_name}_{current_time}.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class DINOTransform:\n",
    "    \"\"\"Implements the global and local view augmentations for DINO [0].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_transforms(self, \n",
    "                       global_crop_size: int = 224,\n",
    "                       global_crop_scale: Tuple[float, float] = (0.4, 1.0),\n",
    "                       local_crop_size: int = 96,\n",
    "                       local_crop_scale: Tuple[float, float] = (0.05, 0.4),):\n",
    "\n",
    "        # first global crop\n",
    "        global_transform_0 = A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.25),\n",
    "            A.VerticalFlip(p=0.25),\n",
    "            A.Resize(global_crop_size, global_crop_size, p=1.0),\n",
    "            A.RandomResizedCrop(height=global_crop_size, width=global_crop_size),\n",
    "            A.Rotate(p=0.5), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "        # second global crop\n",
    "        global_transform_1 = A.Compose(\n",
    "        [\n",
    "            A.Resize(global_crop_size, global_crop_size, p=1.0),\n",
    "            A.RandomResizedCrop(height=global_crop_size, width=global_crop_size),\n",
    "            A.Rotate(p=0.5), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # transformation for the local small crops\n",
    "        local_transform = A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.25),\n",
    "            A.VerticalFlip(p=0.25),\n",
    "            A.Resize(local_crop_size, local_crop_size, p=1.0),\n",
    "            A.RandomResizedCrop(height=local_crop_size, width=local_crop_size),\n",
    "            A.Rotate(p=0.5), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        return [global_transform_0, global_transform_1, local_transform]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import visualize_samples_ssl, DeepLenseDatasetSSL, DeepLenseDatasetSSLRegression\n",
    "\n",
    "# trainset\n",
    "dino_transform = DINOTransform()\n",
    "train_transforms = dino_transform.get_transforms()\n",
    "train_dataset = DeepLenseDatasetSSLRegression(destination_dir = dataset_dir, transforms = train_transforms, mode=\"train\", dataset_name=dataset_name, download=True, channels=1, classes=classes)\n",
    "logging.debug(f\"train data: {len(train_dataset)}\")\n",
    "visualize_samples_ssl(train_dataset, labels_map=classes, num_rows_inner=1, num_cols_inner=2, regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and valid set\n",
    "split_ratio = 0.25  # 0.25\n",
    "valid_len = int(split_ratio * len(train_dataset))\n",
    "train_len = len(train_dataset) - valid_len\n",
    "\n",
    "train_dataset, val_set = random_split(train_dataset, [train_len, valid_len])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "# testset = default_dataset_setup.get_dataset(mode=\"val\")\n",
    "# test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# size check\n",
    "sample = next(iter(train_loader))\n",
    "logging.debug(\"num of classes: \", num_classes)\n",
    "logging.debug(sample[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(torch.nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.backbone[0] = nn.Conv2d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "\n",
    "        self.student_backbone = self.backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOTransformer(torch.nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.student_backbone = self.backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 512, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 512)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create ResNet pretrain model\n",
    "resnet = torchvision.models.resnet50()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "num_ftrs_dict = {\n",
    "    \"resnet18\": 512,\n",
    "    \"resnet34\": 512,\n",
    "    \"resnet50\": 2048,\n",
    "\n",
    "}\n",
    "\n",
    "model =  DINO(backbone, input_dim=num_ftrs_dict[network_type])\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(2, 1, 224, 224), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config = {\n",
    "    \"s1_emb_dim\": 64,  # stage 1 - dimension\n",
    "    \"s1_emb_kernel\": 7,  # stage 1 - conv kernel size\n",
    "    \"s1_emb_stride\": 4,  # stage 1 - conv stride\n",
    "    \"s1_proj_kernel\": 3,  # stage 1 - attention ds-conv kernel size\n",
    "    \"s1_kv_proj_stride\": 2,  # stage 1 - attention key / value projection stride\n",
    "    \"s1_heads\": 2,  # stage 1 - heads\n",
    "    \"s1_depth\": 2,  # stage 1 - depth\n",
    "    \"s1_mlp_mult\": 3,  # stage 1 - feedforward expansion factor\n",
    "    \"s2_emb_dim\": 128,  # stage 2 - (same as above)\n",
    "    \"s2_emb_kernel\": 3,\n",
    "    \"s2_emb_stride\": 2,\n",
    "    \"s2_proj_kernel\": 3,\n",
    "    \"s2_kv_proj_stride\": 2,\n",
    "    \"s2_heads\": 2,\n",
    "    \"s2_depth\": 2,\n",
    "    \"s2_mlp_mult\": 3,\n",
    "    \"mlp_last\": 256,\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_features = 128\n",
    "# # Transformer model\n",
    "# backbone = TransformerModels(\n",
    "#     transformer_type=train_config[\"network_type\"],\n",
    "#     num_channels=train_config[\"channels\"],\n",
    "#     num_classes=in_features,\n",
    "#     img_size=image_size,\n",
    "#     **network_config, # **train_config[\"network_config\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# model = DINOTransformer(backbone, input_dim=in_features)\n",
    "# summary(model, input_size=(2, 1, 224, 224), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "lr = optimizer_config[\"lr\"]\n",
    "weight_decay = optimizer_config[\"weight_decay\"]\n",
    "warmup_epochs = optimizer_config[\"warmup_epoch\"]\n",
    "\n",
    "\n",
    "optimizer_pretrain = optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "warmup_epochs = warmup_epochs\n",
    "num_train_steps = math.ceil(len(train_loader))\n",
    "num_warmup_steps= num_train_steps * warmup_epochs\n",
    "num_training_steps=int(num_train_steps * epochs_pretrain)\n",
    "\n",
    "#learning rate scheduler\n",
    "cosine_scheduler = get_cosine_schedule_with_warmup(optimizer_pretrain,num_warmup_steps = num_warmup_steps,num_training_steps =num_training_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_pretrain = DINOLoss(\n",
    "    output_dim=2048,\n",
    "    warmup_teacher_temp_epochs=5,\n",
    ")\n",
    "# move loss to correct device because it also contains parameters\n",
    "criterion_pretrain = criterion_pretrain.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training\")\n",
    "for epoch in range(epochs_pretrain):\n",
    "    total_loss = 0\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    momentum_val = cosine_schedule(epoch, epochs_pretrain, 0.996, 1)\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        views = batch[:3]\n",
    "        \n",
    "        update_momentum(model.student_backbone, model.teacher_backbone, m=momentum_val)\n",
    "        update_momentum(model.student_head, model.teacher_head, m=momentum_val)\n",
    "        views = [view.to(device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [model.forward_teacher(view) for view in global_views]\n",
    "        student_out = [model.forward(view) for view in views]\n",
    "        loss = criterion_pretrain(teacher_out, student_out, epoch=epoch)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "        # We only cancel gradients of student head.\n",
    "        model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer_pretrain.step()\n",
    "        cosine_scheduler.step()\n",
    "        optimizer_pretrain.zero_grad()        \n",
    "        if ci:\n",
    "            break    \n",
    "            \n",
    "        if batch_idx % 100 == 0:\n",
    "            logging.debug(\n",
    "                f\"Epoch [{epoch}/{epochs_pretrain}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "    if ci:\n",
    "        break    \n",
    "    \n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "\n",
    "        torch.save(model.state_dict(), model_path_pretrained)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    logging.debug(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneClassifier(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super(FinetuneClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.backbone[0](x).flatten(start_dim=1)\n",
    "        z = self.backbone[1](z)\n",
    "        z = self.head(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = model.student_backbone # nn.Sequential(*list(model.backbone.children())[:-1])\n",
    "backbone = nn.Sequential(model.student_backbone, model.student_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head = nn.Sequential(\n",
    "    nn.Linear(2048, 512), # num_ftrs_dict[\"resnet34\"]\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, num_classes),)\n",
    "model = FinetuneClassifier(backbone, classification_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = ... # \"logger/NeurIPS/2023-09-21-18-35-26_Regression_DINO_resnet50_Model_II/checkpoint/resnet50_finetune_Model_II_2023-09-21-18-35-26.pt\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "summary(model, input_size=(2, 1, 224, 224), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "learning_rate = 0.001 # 3e-4\n",
    "weight_decay =  0.01\n",
    "\n",
    "finetune_optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "\n",
    "#optimizer\n",
    "optimizer_finetune = optim.AdamW(model.parameters(), lr=learning_rate) #  weight_decay = 1e-4\n",
    "warmup_epochs = 3\n",
    "num_train_steps = math.ceil(len(train_loader))\n",
    "num_warmup_steps= num_train_steps * warmup_epochs\n",
    "num_training_steps=int(num_train_steps * epochs_finetune)\n",
    "\n",
    "#learning rate scheduler\n",
    "cosine_scheduler = get_cosine_schedule_with_warmup(optimizer_finetune,num_warmup_steps = num_warmup_steps,num_training_steps =num_training_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_finetune = nn.MSELoss()\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepLenseLoss(nn.MSELoss):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, scale_target: float = 1e22, scale_input: float=1.0) -> torch.Tensor:\n",
    "        input = scale_input * input\n",
    "        target = scale_target * target\n",
    "        mse_loss = F.mse_loss(input, target, reduction=self.reduction) \n",
    "        return  mse_loss\n",
    "\n",
    "criterion_finetune = DeepLenseLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune\n",
    "finetune_regression(\n",
    "    epochs_finetune,\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    criterion_finetune,\n",
    "    optimizer_finetune,\n",
    "    finetuned_model_path,\n",
    "    valid_loader=val_loader,\n",
    "    scheduler=cosine_scheduler,\n",
    "    ci=ci,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = dataset_dir\n",
    "test_dataset = DeepLenseDatasetSSLRegression(destination_dir = test_dataset_dir, transforms = train_transforms, mode=\"test\", dataset_name=dataset_name, download=True, channels=1, classes=classes)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "path = ... # /logger/2023-09-21-18-35-26_Regression_DINO_resnet50_Model_II/checkpoint/resnet50_finetune_Model_II_2023-09-21-18-35-26.pt\" \n",
    "model.load_state_dict(torch.load(os.path.join(path)))\n",
    "model.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: combine all classes\n",
    "class InferenceRegressionSSL(InferenceABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        best_model: nn.Module,\n",
    "        test_loader: DataLoader,\n",
    "        device: str,\n",
    "        num_classes: int,\n",
    "        testset: Dataset,\n",
    "        dataset_name: str,\n",
    "        labels_map: dict,\n",
    "        image_size: int,\n",
    "        channels: int,\n",
    "        log_dir: str,\n",
    "        criterion,\n",
    "        destination_dir=\"data\",\n",
    "        current_time=None,\n",
    "        \n",
    "    ) -> None:\n",
    "        self.criterion = criterion\n",
    "        super().__init__(\n",
    "            best_model,\n",
    "            test_loader,\n",
    "            device,\n",
    "            num_classes,\n",
    "            testset,\n",
    "            dataset_name,\n",
    "            labels_map,\n",
    "            image_size,\n",
    "            channels,\n",
    "            log_dir,\n",
    "            destination_dir,\n",
    "            current_time,\n",
    "        )\n",
    "\n",
    "        \"\"\"Class for infering the trained model. \\n\n",
    "        Plots `Confusion matrix`, computes `AUC` and `ROC` score.  `normalize=True`\n",
    "\n",
    "        Args:\n",
    "            best_model (nn.Module): best trained model to infer\n",
    "            test_loader (DataLoader): pytorch loader for testset\n",
    "            device (Union[int, str]): number or name of device\n",
    "            num_classes (int): # of classes for classification\n",
    "            testset (Dataset): dataset for testing\n",
    "            dataset_name (str): name of testeset\n",
    "            labels_map (dict): dict for mapping labels to number e.g `{0: \"axion\"}`\n",
    "            image_size (int): size of input image\n",
    "            channels (int): # of channels of input image\n",
    "            log_dir (str): directory for saving logs\n",
    "            destination_dir (str, optional): directory where data is saved. Defaults to \"data\".\n",
    "\n",
    "        Example:\n",
    "        >>>     infer_obj = Inference(\n",
    "        >>>             best_model= model,\n",
    "        >>>             test_loader= test_loader,\n",
    "        >>>             device=device,\n",
    "        >>>             num_classes=num_classes,\n",
    "        >>>             testset=testset,\n",
    "        >>>             dataset_name=dataset_name,\n",
    "        >>>             labels_map=classes,\n",
    "        >>>             image_size=image_size,\n",
    "        >>>             channels=train_config[\"channels\"],\n",
    "        >>>             destination_dir=\"data\",\n",
    "        >>>             log_dir=log_dir)\n",
    "        \"\"\"\n",
    "\n",
    "    def infer(self):\n",
    "        \"\"\"Plots `ROC` curve\"\"\"\n",
    "        print(\"Inference started ...\")\n",
    "        total = 0\n",
    "        all_test_loss = []\n",
    "        all_test_accuracy = []\n",
    "        self.label_true_arr = []\n",
    "        self.label_true_arr_onehot = []\n",
    "        self.label_pred_arr = []\n",
    "        self.pred_arr = []\n",
    "        self.epoch_loss = 0\n",
    "        plt.rcParams.update(plt.rcParamsDefault)\n",
    "        fig = plt.figure()\n",
    "\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            self.best_model.eval()\n",
    "            for batch_idx, batch in enumerate(self.test_loader):\n",
    "                x = batch[0].to(self.device)\n",
    "                label = batch[-1].to(self.device)\n",
    "                output = self.best_model(x)\n",
    "                loss = self.criterion(output, label)\n",
    "                self.epoch_loss += loss\n",
    "\n",
    "                self.label_pred_arr.append(output.cpu().numpy().flatten())\n",
    "                self.label_true_arr.append(label.cpu().numpy())\n",
    "\n",
    "                # total += t.shape[0]\n",
    "                # correct += (prediction == t).sum().item()\n",
    "            self.epoch_loss = self.epoch_loss / len(self.test_loader)\n",
    "            \n",
    "        self.label_pred_arr = np.concatenate(self.label_pred_arr, axis=0)\n",
    "        self.label_true_arr = np.concatenate(self.label_true_arr, axis=0)\n",
    "\n",
    "    def plot_scatter(self):\n",
    "        print(\"MAE loss: \", self.epoch_loss)\n",
    "        plt.scatter(self.label_true_arr, self.label_pred_arr, color=\"black\")\n",
    "\n",
    "        plt.title(\"Validation Set\")\n",
    "        plt.xlabel(\"Observed mass\")\n",
    "        plt.ylabel(\"Predicted mass\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "infer_obj = InferenceRegressionSSL(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device,\n",
    "    num_classes,\n",
    "    val_set,\n",
    "    dataset_name,\n",
    "    labels_map=classes, # classes\n",
    "    image_size=image_size,\n",
    "    channels=channels,\n",
    "    destination_dir=\"data\",\n",
    "    log_dir=log_dir,  # log_dir\n",
    "    criterion= criterion_finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_obj.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_obj.plot_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(infer_obj.label_true_arr, infer_obj.label_pred_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
